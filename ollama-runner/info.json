{
  "name": "ollama",
  "version": "1.0.0",
  "description": "Ollama - Run Large Language Models locally",
  "author": "FunctionLand",
  "license": "MIT",
  "category": "AI/ML",
  "tags": ["ai", "llm", "machine-learning", "gpu"],
  "icon": "ollama-icon.png",
  "homepage": "https://ollama.ai",
  "repository": "https://github.com/jmorganca/ollama",
  "requirements": {
    "min_memory": "8GB",
    "min_storage": "10GB",
    "gpu_optional": true
  },
  "ports": [
    {
      "internal": 11434,
      "external": 11434,
      "protocol": "tcp",
      "description": "Ollama API endpoint"
    }
  ],
  "environment": {
    "OLLAMA_HOST": "0.0.0.0",
    "OLLAMA_MODELS": "/data/ollama/models",
    "OLLAMA_NUM_PARALLEL": "1",
    "OLLAMA_MAX_LOADED_MODELS": "1"
  }
}